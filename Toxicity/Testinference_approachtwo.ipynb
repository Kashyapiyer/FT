{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "87D_uNSumeA0"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = '/content/drive/MyDrive/debertatftmdl'\n",
        "test_cases = [\n",
        "      \"beautiful movie.\",\n",
        "      \"This is a beautiful movie\",\n",
        "      \"This is such a hateful and horrible movie\",\n",
        "      \"You are an asshole\",\n",
        "      \"wonderful person\",\n",
        "      \"You are a wonderful person!\",\n",
        "      \"I hate you and wish you would die\",\n",
        "      \"The weather is nice today\",\n",
        "      \"You're completely stupid and useless\",\n",
        "      \"You idiot you will pay for this\",\n",
        "      \"You are rotten bloody person\",\n",
        "      \"You should be ashamed of yourself\",\n",
        "      \"You should die or hang yourself why are your still living\"\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aObLrUrQrCcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haEwoi54rBEg",
        "outputId": "5e5c364f-7996-4b4f-91f7-fcfbaefd137c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCkFh6DBrGdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import torch, torch.nn as nn\n",
        "import pandas as pd\n",
        "from transformers import  AutoConfig, AutoModel\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DebertaForSequenceClassification\n",
        "\n",
        "class ToxicityPredictor:\n",
        "    def __init__(self, model_path=OUTPUT_DIR):\n",
        "        print(model_path)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path, config=config, ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Move to the selected device\n",
        "        #self.model.to(self.device)\n",
        "\n",
        "\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer)) #add this line\n",
        "\n",
        "\n",
        "        # Ensure model is in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Store the original state\n",
        "        self.original_state = deepcopy(self.model.state_dict())\n",
        "\n",
        "    def predict(self, text, threshold=0.5):\n",
        "        # Ensure model is in eval mode before each prediction\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=128,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            #.to(self.device)\n",
        "\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "            #outputs = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "            print(outputs[0])\n",
        "\n",
        "            print(\"### logits####\")\n",
        "            print(outputs.logits)\n",
        "            print(\"##################\")\n",
        "\n",
        "\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "            print(\"####### probabilities ######\")\n",
        "            print(probabilities)\n",
        "\n",
        "            toxic_prob = probabilities[0][1].item()\n",
        "            print(\"### toxic_prob ####\")\n",
        "            print(toxic_prob)\n",
        "\n",
        "            prediction = 'Toxic' if toxic_prob >= threshold else 'Non-toxic'\n",
        "\n",
        "            return {\n",
        "                'text': text,\n",
        "                'prediction': prediction,\n",
        "                'toxic_probability': f\"{toxic_prob:.3f}\",\n",
        "                'non_toxic_probability': f\"{1-toxic_prob:.3f}\",\n",
        "                'raw_probabilities': probabilities[0].cpu().numpy()\n",
        "            }\n",
        "\n",
        "    def reset_model(self):\n",
        "        \"\"\"Reset model to original state\"\"\"\n",
        "        self.model.load_state_dict(self.original_state)"
      ],
      "metadata": {
        "id": "YBYwtJCamgb4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_consistency(model_path, test_cases):\n",
        "    predictor = ToxicityPredictor(model_path)\n",
        "    intialrundict = {'contextstr': [], 'ToxicProbability': [], 'predictionresult': []}\n",
        "    # First run\n",
        "    print(\"First run:\")\n",
        "    for text in test_cases:\n",
        "        t = predictor.predict(text)\n",
        "        intialrundict['contextstr'].append(text)\n",
        "        intialrundict['ToxicProbability'].append(t['toxic_probability'])\n",
        "        intialrundict['predictionresult'].append(t['prediction'])\n",
        "    intialresultdf = pd.DataFrame(intialrundict)\n",
        "    print(intialresultdf.head(15))\n",
        "\n",
        "    print(\"################################################\")\n",
        "\n",
        "    # Reset model\n",
        "    predictor.reset_model()\n",
        "\n",
        "    # Second run\n",
        "    print(\"\\nSecond run:\")\n",
        "    secndrundict = {'contextstr': [], 'ToxicProbability': [], 'predictionresult': []}\n",
        "    for text in test_cases:\n",
        "        test = predictor.predict(text)\n",
        "        secndrundict['contextstr'].append(text)\n",
        "        secndrundict['ToxicProbability'].append(test['toxic_probability'])\n",
        "        secndrundict['predictionresult'].append(test['prediction'])\n",
        "\n",
        "    secondresultdf = pd.DataFrame(secndrundict)\n",
        "    print(secondresultdf.head(15))"
      ],
      "metadata": {
        "id": "jFk6e0P_mnKg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_consistency(model_path=OUTPUT_DIR, test_cases=test_cases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP2JfpOEmpUi",
        "outputId": "2c27bfd8-a0fd-42a2-99fd-06a72a1a4908"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/debertatftmdl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/debertatftmdl and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/debertatftmdl and are newly initialized because the shapes did not match:\n",
            "- deberta.embeddings.word_embeddings.weight: found shape torch.Size([128100, 768]) in the checkpoint and torch.Size([50265, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First run:\n",
            "                                           contextstr ToxicProbability  \\\n",
            "0                                    beautiful movie.            0.455   \n",
            "1                           This is a beautiful movie            0.458   \n",
            "2           This is such a hateful and horrible movie            0.468   \n",
            "3                                  You are an asshole            0.465   \n",
            "4                                    wonderful person            0.437   \n",
            "5                         You are a wonderful person!            0.447   \n",
            "6                   I hate you and wish you would die            0.448   \n",
            "7                           The weather is nice today            0.457   \n",
            "8                You're completely stupid and useless            0.447   \n",
            "9                     You idiot you will pay for this            0.460   \n",
            "10                       You are rotten bloody person            0.440   \n",
            "11                  You should be ashamed of yourself            0.452   \n",
            "12  You should die or hang yourself why are your s...            0.460   \n",
            "\n",
            "   predictionresult  \n",
            "0         Non-toxic  \n",
            "1         Non-toxic  \n",
            "2         Non-toxic  \n",
            "3         Non-toxic  \n",
            "4         Non-toxic  \n",
            "5         Non-toxic  \n",
            "6         Non-toxic  \n",
            "7         Non-toxic  \n",
            "8         Non-toxic  \n",
            "9         Non-toxic  \n",
            "10        Non-toxic  \n",
            "11        Non-toxic  \n",
            "12        Non-toxic  \n",
            "################################################\n",
            "\n",
            "Second run:\n",
            "                                           contextstr ToxicProbability  \\\n",
            "0                                    beautiful movie.            0.455   \n",
            "1                           This is a beautiful movie            0.458   \n",
            "2           This is such a hateful and horrible movie            0.468   \n",
            "3                                  You are an asshole            0.465   \n",
            "4                                    wonderful person            0.437   \n",
            "5                         You are a wonderful person!            0.447   \n",
            "6                   I hate you and wish you would die            0.448   \n",
            "7                           The weather is nice today            0.457   \n",
            "8                You're completely stupid and useless            0.447   \n",
            "9                     You idiot you will pay for this            0.460   \n",
            "10                       You are rotten bloody person            0.440   \n",
            "11                  You should be ashamed of yourself            0.452   \n",
            "12  You should die or hang yourself why are your s...            0.460   \n",
            "\n",
            "   predictionresult  \n",
            "0         Non-toxic  \n",
            "1         Non-toxic  \n",
            "2         Non-toxic  \n",
            "3         Non-toxic  \n",
            "4         Non-toxic  \n",
            "5         Non-toxic  \n",
            "6         Non-toxic  \n",
            "7         Non-toxic  \n",
            "8         Non-toxic  \n",
            "9         Non-toxic  \n",
            "10        Non-toxic  \n",
            "11        Non-toxic  \n",
            "12        Non-toxic  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_consistency(model_path=OUTPUT_DIR, test_cases=[\"You should die or hang yourself\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiiqHrm5spyy",
        "outputId": "0d115002-b54f-4942-adf5-8d0ec8bb90c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/debertatftmdl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/debertatftmdl and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.self.v_bias', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/debertatftmdl and are newly initialized because the shapes did not match:\n",
            "- deberta.embeddings.word_embeddings.weight: found shape torch.Size([128100, 768]) in the checkpoint and torch.Size([50265, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First run:\n",
            "tensor([[-0.0389, -0.0259]])\n",
            "### logits####\n",
            "tensor([[-0.0389, -0.0259]])\n",
            "##################\n",
            "####### probabilities ######\n",
            "tensor([[0.4968, 0.5032]])\n",
            "### toxic_prob ####\n",
            "0.5032382607460022\n",
            "                        contextstr ToxicProbability predictionresult\n",
            "0  You should die or hang yourself            0.503            Toxic\n",
            "################################################\n",
            "\n",
            "Second run:\n",
            "tensor([[-0.0389, -0.0259]])\n",
            "### logits####\n",
            "tensor([[-0.0389, -0.0259]])\n",
            "##################\n",
            "####### probabilities ######\n",
            "tensor([[0.4968, 0.5032]])\n",
            "### toxic_prob ####\n",
            "0.5032382607460022\n",
            "                        contextstr ToxicProbability predictionresult\n",
            "0  You should die or hang yourself            0.503            Toxic\n"
          ]
        }
      ]
    }
  ]
}
